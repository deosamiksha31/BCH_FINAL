# -*- coding: utf-8 -*-
"""Final_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OgDY1vI_06R6ajdGjhvFFvCYZaXN2iae
"""

# There are two main classifications of tumors. One is known as benign and the other as malignant. A benign tumor is a tumor that does not invade its surrounding tissue or spread around the body. A malignant tumor is a tumor that may invade its surrounding tissue or spread around the body.

!pip install opencv-python-headless

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!pip install tensorflow

# import libraries
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import cv2
import os
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential

from google.colab import drive
drive.mount('/content/drive', force_remount=True)  # Force remount to refresh Drive

# List available files to verify path
!ls "/content/drive/My Drive/"

# Verify if folder_name exists
!ls "/content/drive/My Drive/folder_name/"

# Copy the file to Colab's working directory
!cp "/content/drive/My Drive/folder_name/Images.zip" /content/

# Verify if the file was copied successfully
!ls /content/

# Extract the ZIP file
!unzip /content/Images.zip -d /content/extracted_files/

# Verify extracted files
!ls /content/extracted_files/

# Install gdown
!pip install gdown

# Import gdown
import gdown

# Correctly define the file ID (extracted from the link)
file_id = '1HmtBIKMDyM_Hb2O3IykGFHsICN_SdTBl'

# Construct the correct Google Drive download URL
url = f'https://drive.google.com/uc?id={file_id}'

# Download the file and save it as 'Images.zip'
gdown.download(url, 'Images.zip', quiet=False)

# Unzip the file
!unzip Images.zip -d /content/extracted_files/

# List extracted files
!ls /content/extracted_files/

# Data loading
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(rescale=1.0/255.0)
train_data = datagen.flow_from_directory('/content/extracted_files/Images', target_size=(224, 224), batch_size=32, class_mode='binary')

# Visual inspection of the images (view a sample of images)
import matplotlib.pyplot as plt
import numpy as np

images, labels = next(train_data)
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(images[i])
    plt.title(f"Label: {labels[i]}")
    plt.axis('off')
plt.show()

# Image dimensions and shapes
image_shape = images[0].shape
print(f"Image shape: {image_shape}")

# Define the path where data is extracted
extract_path = "/content/extracted_files/Images"

# Ensure the path exists
if not os.path.exists(extract_path):
    print(f"Error: The directory '{extract_path}' does not exist.")
else:
    # Get the class names (folder names)
    class_counts = {}

    for class_name in os.listdir(extract_path):
        class_folder = os.path.join(extract_path, class_name)

        # Count only if it's a directory
        if os.path.isdir(class_folder):
            class_counts[class_name] = len(os.listdir(class_folder))

    # Convert to Pandas Series for plotting
    class_counts_series = pd.Series(class_counts)

    # Plot the class distribution
    plt.figure(figsize=(10, 5))  # Adjust figure size
    sns.barplot(x=class_counts_series.index, y=class_counts_series.values)
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.title('Class Distribution')
    plt.xticks(rotation=45)  # Rotate class labels if long
    plt.show()

# Check duplicates or corrupt images

from PIL import Image

# Define the directory containing images
directory = "/content/extracted_files/Images"

# Collect all image file paths from the directory
image_paths = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(('png', 'jpg', 'jpeg'))]

# Initialize a set to store unique image hashes
image_hashes = set()

# Check if image_paths is empty
if not image_paths:
    print("No images found in the specified directory.")
else:
    for image_path in image_paths:
        try:
            image = Image.open(image_path)
            # Process the image
            print(f"Processing: {image_path}")
        except Exception as e:
            print(f"Error processing {image_path}: {e}")

# Define image directory
image_dir = "/content/extracted_files/Images"

# Set image size
IMG_SIZE = (128, 128)

# Initialize data and labels
data = []
labels = []
image_classes = os.listdir(image_dir)  # Class names
label_map = {label: idx for idx, label in enumerate(image_classes)}  # Assign labels

# Load images
for class_name in image_classes:
    class_path = os.path.join(image_dir, class_name)
    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.resize(img, IMG_SIZE)  # Resize
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
                data.append(img)
                labels.append(label_map[class_name])

# Convert lists to NumPy arrays
data = np.array(data, dtype="float32") / 255.0  # Normalize images
labels = np.array(labels)

print("Data Loaded Successfully!")

# Image preprocessing and augmentation
datagen = ImageDataGenerator(
    rescale=1.0/255.0,  # Normalize pixel values
    rotation_range=20,   # Random rotation
    width_shift_range=0.2,  # Horizontal shift
    height_shift_range=0.2,  # Vertical shift
    shear_range=0.2,  # Shearing transformations
    zoom_range=0.2,   # Random zoom
    horizontal_flip=True,  # Flip images horizontally
    fill_mode='nearest'  # Fill pixels after transformation
)

from sklearn.model_selection import train_test_split

# Split the dataset (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42, stratify=labels)

print("Training set size:", X_train.shape)
print("Testing set size:", X_test.shape)



import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Example dataset (you can replace this with your actual data)
X = np.random.rand(1000, 20)
y = np.random.randint(2, size=1000)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_acc = accuracy_score(y_test, rf_model.predict(X_test))

# Keras Model
keras_model = Sequential([
    Dense(64, activation='relu', input_shape=(20,)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = keras_model.fit(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)

# Get the best accuracy
keras_acc = max(history.history['val_accuracy'])

# Compare Accuracies
print(f"Random Forest Accuracy: {rf_acc * 100:.2f}%")
print(f"Keras Model Accuracy: {keras_acc * 100:.2f}%")

# Show how accuracy increases
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Keras Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Keras Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Flatten images for Random Forest
X_train_rf = X_train.reshape(X_train.shape[0], -1)
X_test_rf = X_test.reshape(X_test.shape[0], -1)

# Train Random Forest Model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_rf, y_train)

# Make predictions
rf_preds = rf_model.predict(X_test_rf)

# Evaluate Model
rf_accuracy = accuracy_score(y_test, rf_preds)
print("Random Forest Accuracy:", rf_accuracy)
print("Classification Report:\n", classification_report(y_test, rf_preds))

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Load and preprocess the data
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize and reshape the data
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build the CNN model
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Load and preprocess the data
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Normalize and reshape the data
X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build the CNN model
cnn_model = model = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)

# Get the highest accuracy
max_acc = max(history.history['val_accuracy'])
print(f"CNN Validation Accuracy: {max_acc * 100:.2f}%")

# Plot accuracy over epochs
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Save the trained model
cnn_model.save("cnn_model.h5")
print("Model saved as cnn_model.h5")